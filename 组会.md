# 组会记录

记录组会时间、主讲人、内容、问题、讨论、启示、后续跟进等。

## 2024.8.12 SLAM 

- 主讲人：丁磊

- 内容：

  - 背景：

    - ICRA论文ONek-SLAM 
    - 基于神经辐射场、关键点
    - 基于Nerf，提高密集SLAM的重建精度
    - 大多缺乏细粒度细节，且对光照变化敏感
    - SIFT在光变条件下具有鲁棒性
    - 因此，本文联合特征点和对象级的Nerf，构建SLAM系统

  - 论文方法：

    - 输入RGB图像和跨帧对象级掩码

    - 对象级位姿估计：用SIFT从图像中提取特征点，并对用对象掩码定位到特征点属于哪个对象，*计算重投影误差？*优化位姿，位姿估计得到轨迹，初始帧和后续运动帧的特征点匹配，匹配到同一个物体，得到旋转平移变化矩阵，从而算出轨迹。

    - 对象级Nerf优化：将场景分割，每个对象由边界框和掩码定义，对每个对象选取像素点，使用NeRF模型预测其在世界坐标系中的颜色和占有率，并计算损失函数。

    - 自适应场景管理：提高鲁棒性

      - **==通过计算重投影误差和光度/深度损失函数，识别和排除动态物体==**

      - 加入一个参数$ \lambda $ 调节权重

        *怎么识别光照条件不好？黑夜、颠簸*（可能有规律模型）
    
  - 实验结果

    - DataSet: Replica, ScanNet, TUM RGB-D
  - *评价指标？* 针对3D重建和对象级识别方面，有哪些特殊的评价指标
  
- 提问：
    - 加激光雷达数据会不会更好抵御光照影响？将深度图像信息换成激光雷达点云去做
  - 为什么想讲这篇论文？
    - 离线训练的还是在线训练的？边训练边预测？
    - NeRF的作用是什么？
  - 如果RGB色彩分布，很丰富，特征点很多，那如何进行NeRF？特征点多，匹配可能越好





2024.12.14

曲雅云

语义分割网络的研究

TP-dataset 盲道分割 2700多张图片

- yolo做语义分割。效果一般

- bisenetv分割网络来做，想替换其backbone，参考其他网络如mobilenet

  

15种障碍物的检测 

提议：做完整，基于盲道参考的导航，再把避障糅合进来，部署在盲杖上，做实验可以先从一小段路开始，比较普通盲杖和智能盲杖，看看是否智能盲杖的加入，使得走的又直又快。



2025.7.8 

窦瑞琪： NGEL-SLAM   ICRA 2024 

1.文献信息  

- 传统方法缺点：点云地图稀疏
- 早期神经隐式
- 本文优点：全局一致、低延迟

2. 系统架构

- 模块构成：
  - 跟踪模块：
  - 映射模块：动态局部映射、子地图、
  - 回环模块：两阶段优化，有回环优化后，地图效果更精确
- 基于八叉树的隐式神经， 用这个数据结构，提高了内存效率。

3.实验结果 

- Replica数据集， 稠密重建质量对比
- ScanNet数据集，真实大场景（？）挑战
- 运行时间
- 消融实验



问题：

1.  真实大场景？ 数据量大？

2.  GT是什么样的格式保存的？不同数据集的GT的格式一样吗？ GT是怎么获取的？

3.  图像重建的方法，Nerf 和 隐式神经有什么区别？
4.  为什么用八叉树？（可能是因为立方体的8个顶点）特征从哪里获取来的？ 优点是什么？与传统的SLAM八叉树的区别？ 
5.  每一帧输出的是什么？ 回环检测调整的是哪个地图？回环检测最后怎么影响最终效果？子地图拼接和回环检测有什么关系？怎么把回环检测和稠密建图都用上了？ 锚点是什么（可能是一段时间内的关键帧的关键点）？
6. 实时性怎么样？为什么没有对比ORB-SLAM3？
7. 子地图为什么要有？ 误差小，好修改；  以什么划分的子地图？根据共视关系（重投影误差）、根据语义； 子图怎么拼接起来？

8. 用于地下停车场？（环境变化不大）  看到空调的品牌和型号信息，这个先验信息用来获取空调的尺寸信息，那么可以用来修正地图。 根据比例来修正。 或者怎么去修正？ 这些信息很有用。 基于场景理解的先验矫正。

   昆明池采数据，用已有的雕塑的尺寸去修正建图。 利用汽车、共享单车等品牌信息，尺寸等去矫正建图。 雪天减速带利用红绿灯斑马线等先验信息去预测。  

   

   导航栏PPT不错



巩：从创新点讲工作进展 ，基于AI智能识别、交互，融合多传感器的复合恶劣环境下的SLAM

1.  视觉SLAM、激光SLAM、IMU各自缺点
2.  复合恶劣环境：比如夜间颠簸下雪
3.  系统图
4. 四个模块
   1. 交互模块
   2. Radar和LiDar的融合模块
   3. 视觉：RGB 和 红外二选一
   4. IMU自适应低通滤波
5. 实验效果，视觉看上去的效果、 实时性、ATE、RPE等； 在自建数据集和南洋理工大学的数据集上检测。  在雾里的效果。

6. 总结：在动态物体和回环检测方面仍有挑战。



问题：

1. 用户交互具体什么样的？ 用户可能自己也不知道想要的东西？什么样的用户？ 意义？ AI有什么用？（识别场景）
2.  红外在哪里融合的？
3.  时间同步怎么实现的？
4.  用激光点去找4D雷达点？  反过来算法更快。 

5.  固定旋转优先平移 什么意思？为什么要做实时外参标定？
6.  融合是有权重吗？
7.  滤波是什么？过滤噪声点
8.  AI的初识阈值是怎么来的？