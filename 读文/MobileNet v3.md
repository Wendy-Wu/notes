# MobileNet v3

## 1. 2019年Google提出的轻量级神经网络结构

旨在在移动设备上实现高效的图像识别和分类。

## 2. 主要包括以下几个关键组件

- 基础模块 Base Module：使用了一种“倒残差”的基础模块结构。该结构采用了深度可分离卷积和线性瓶颈，以**减少参数数量和计算复杂度**。
- Squeeze-and-Excitation模块：通过学习通道之间的相互关系，动态地调整通道权重，以增强模型的表征能力。
- Hard-Swish激活函数：与传统的ReLu激活函数相比，具有更快的计算速度和更好的性能。
- 在网络结构上进行了优化，包括通过网络宽度和分辨率的动态调整，以适应不同的计算资源和任务需求。



## 3. 前向传播

前向传播给出神经网络对给定输入的预测结果，这是神经网络用于推理时的主要步骤。就是从输入到输出，计算的过程。如代码：

```python
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.layer1 = nn.Linear(10, 5)  # 输入维度为10，输出维度为5
        self.layer2 = nn.Linear(5, 2)   # 输入维度为5，输出维度为2
        
def forward(self, x):
    x = self.layer1(x)  # 第1层：全连接层
    x = torch.relu(x)   # 使用ReLU激活函数
    x = self.layer2(x)  # 第2层：全连接层
    return x            # 返回最终输出
```

**第一步**：`x = self.layer1(x)`：输入 `x` 通过第一层，执行矩阵乘法 `W1 * x + b1`，并得到一个新的张量。

**第二步**：`x = torch.relu(x)`：通过激活函数 `ReLU`，将非线性引入网络。

**第三步**：`x = self.layer2(x)`：通过第二层执行相同的线性运算，得到最终的输出。

**返回值**：`forward` 函数的最后会返回最终计算的输出结果，这就是前向传播得到的模型预测值。



## 4. 批量归一化

**归一化处理**（Normalization）是指将数据的数值范围转换为一个特定的范围，通常是 `[0, 1]` 或 `[-1, 1]`，使数据具有相似的尺度或范围，从而提高模型的性能和收敛速度。



如果不同特征的数值范围差别很大（例如，有的特征值在0-1之间，而有的特征值可以达到上千甚至更大），这些差异会导致模型训练过程中某些特征对损失函数的影响过大，而其他特征影响太小，从而降低模型的效果。

神经网络中的激活函数（如`sigmoid`、`tanh`等）在输入值较大或较小时可能会饱和，导致梯度消失问题，使得网络的训练变得困难。归一化后，输入值的范围被限制在合理的区间内，网络能够更稳定地训练。

- 归一化的应用场景：
  - **图像处理**：图像像素值通常在 `[0, 255]` 之间，将其归一化到 `[0, 1]` 或 `[-1, 1]` 之间有助于神经网络更快收敛。
  - **深度学习中的批归一化（Batch Normalization）**：在训练神经网络时，常常在每个批次内对数据进行归一化，防止梯度消失或爆炸，保证训练的稳定性。
  - **数据预处理**：在传统的机器学习模型中（如SVM、KNN、线性回归等），归一化处理是常规的预处理步骤，尤其是在特征具有不同量纲时，归一化非常重要。
- 常见的归一化方法：
  - **Min-Max 归一化**：
    - **公式**：将数据缩放到 `[0, 1]` 范围。 $⁡x' = \frac{x - x_{\min}}{x_{\max} - x_{\min}}$
    - 其中，`x` 是原始数据，`x'` 是归一化后的数据，`x_{\min}` 和 `x_{\max}` 分别是数据集的最小值和最大值。
    - **适用场景**：当数据具有已知的最大值和最小值时适用，比如图像数据。
  - **Z-score 归一化（标准化）**：
    - **公式**：将数据变换为均值为0，标准差为1的标准正态分布。$ x' = \frac{x - \mu}{\sigma}$
    - 其中，`x` 是原始数据，`x'` 是标准化后的数据，`μ` 是数据的均值，`σ` 是标准差。
    - **适用场景**：适合数据分布接近正态分布的场景，且不确定最大或最小值的情况下。
  - **最大绝对值归一化**：
    - **公式**：将数据缩放到 `[-1, 1]` 之间。 $x' = \frac{x}{|x_{\max}|}$
    - 其中 `x_{\max}` 是数据集中绝对值最大的元素。
    - **适用场景**：适用于数据中心对称于0，且包含负值的情况。
  - **L2 归一化（向量归一化）**：
    - **公式**：将数据的每个特征值除以该数据的欧几里得范数（L2范数）。 $x' = \frac{x}{\|x\|}$
    - **适用场景**：多用于文本或向量数据处理（如词嵌入、文档嵌入等），将向量长度标准化为1。



## 5. 恒等映射

在残差块中经常用到。

```python
class ResidualBlock(nn.Module):
    def __init__(self, inp, oup, stride):
        super(ResidualBlock, self).__init__()
        self.conv = nn.Conv2d(inp, oup, kernel_size=3, stride=stride, padding=1)
        self.bn = nn.BatchNorm2d(oup)
        self.relu = nn.ReLU(inplace=True)
           
   # 判断是否可以直接跳过卷积
    self.identity = stride == 1 and inp == oup

def forward(self, x):
    out = self.conv(x)
    out = self.bn(out)
    
    if self.identity:
        out = out + x  # 恒等映射，跳过卷积层
    return self.relu(out)
```

### 应用场景：

- 在**残差网络**中，通常会引入**恒等映射**（identity mapping），通过跳跃连接将输入直接加到输出上。这样的结构可以缓解梯度消失问题，使得更深层的网络也能被有效训练。
- **`stride == 1` 且 `inp == oup`** 的情况下，网络可以跳过对输入的卷积运算，将输入直接与卷积输出相加。这种恒等映射确保了信息可以直接从输入传递到输出，有助于特征的保留。



## 6. 3*3卷积操作，会改变图片大小吗

**3x3卷积操作**是否改变图片大小，取决于以下几个关键参数：

1. **步长（stride）**：卷积核滑动的步幅。
2. **填充（padding）**：是否在输入图片的边缘补充像素。
3. **卷积核大小（kernel size）**：在此情况下是3x3。

- **没有填充的情况（padding=0）**：

如果不进行填充，使用 3x3 卷积核，图片的尺寸**会减小**。假设输入图片的大小是 W×H，经过一次 3x3 卷积操作，输出图片的尺寸将使图片的宽度和高度都减少了 2。比如，对于一个 32×32的输入图片，经过一次 3x3 卷积，输出将是 30×3030 \。

-  **有填充的情况（padding=1）**：

为了保持卷积前后图片的大小一致，通常会使用填充。在 3x3 卷积中，常用的填充方式是将输入的四周都补充 1 行/列的像素，称为 `padding=1`。这样卷积核在滑动时可以覆盖边缘像素，输出图片的尺寸可以与输入图片保持相同。

#### 总结：

- **没有填充，stride=1**：输出图片的尺寸会变小。
- **有填充（padding=1），stride=1**：输出图片的尺寸保持不变。
- **有填充，但 stride>1**：输出图片尺寸会变小。



## 7.池化

**池化（Pooling）\**是一种常用于卷积神经网络（CNN）中的操作，它的主要目的是\**下采样**特征图，减小尺寸的同时保留重要特征，从而减少计算量并防止过拟合



## 8. mobilenetv3的卷积核在哪里定义的？

在 MobileNetV3 中，卷积核通常在 `MobileNetV3Block` 或类似的模块类中定义。以 `MobileNetV3` 的实现为例，卷积核的定义通常会在 `__init__` 方法中使用 `nn.Conv2d` 类。

`nn.Conv2d` 的参数中包含了卷积核的大小、步长、填充和输入输出通道数等：

- `inp`：输入通道数。
- `oup`：输出通道数。
- `kernel_size`：卷积核的大小。
- `stride`：步长，决定卷积核每次移动的距离。
- `padding`：填充，用于控制输出特征图的大小。
- `groups`：用于分组卷积，通常在深度可分离卷积（Depthwise Convolution）中使用，设为输入通道数的值表示深度可分离卷积。

**卷积核的数量**

在 `nn.Conv2d` 中，卷积核的数量由以下参数控制：

- **`out_channels`**：输出通道数，表示要生成的卷积核数量。每个卷积核将生成一个特征图（feature map），因此输出通道数即为卷积核的数量。比如：`out_channels=64` 表示会有 64 个卷积核。

**卷积核的学习**

在训练过程中，卷积核的权重会根据反向传播算法不断更新。卷积核的初始值可以使用随机初始化，但最终的卷积核值将根据模型在特定任务上的性能进行优化。因此，卷积核的“形状”或“样子”在训练过程中是动态变化的。

